## Exploratory Data Analysis

### Using Spark
~~~
bin/spark-shell --master spark://localhost:7077 \
--packages com.databricks:spark-csv_2.10:1.3.0 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer

import org.apache.spark.rdd.RDD
import org.apache.spark.storage.StorageLevel

val rawRDD = sc.textFile("/work/R/example/stocks/zyadoEUR.csv")

rawRDD.take(10).foreach(println)

1398372911,341.740000000000,0.150000000000
1398373075,341.740000000000,0.050000000000
1398377372,341.750000000000,0.142000000000
1398377372,341.750000000000,0.008000000000
1398384744,341.770000000000,0.012000000000
1398384744,341.750000000000,0.078000000000
1398384777,341.770000000000,0.090000000000
1398385616,341.740000000000,0.070000000000
1398386034,341.740000000000,0.060000000000
1398386798,341.770000000000,0.003000000000

case class Ticker(ts: Long, price: Float, volume: Float)

val df = rawRDD.map(_.split(",")).map(row => Ticker(row(0).toLong, row(1).toFloat, row(2).toFloat)).toDF()

df.cache()

df.show()

+----------+------+------+
|        ts| price|volume|
+----------+------+------+
|1398372911|341.74|  0.15|
|1398373075|341.74|  0.05|
|1398377372|341.75| 0.142|
|1398377372|341.75| 0.008|
|1398384744|341.77| 0.012|
|1398384744|341.75| 0.078|
|1398384777|341.77|  0.09|
|1398385616|341.74|  0.07|
|1398386034|341.74|  0.06|
|1398386798|341.77| 0.003|
|1398386798|341.87| 0.021|
|1398387965|342.54| 0.025|
|1398388834| 343.0| 0.123|
|1398436109| 343.0| 0.041|
|1398456998|343.54|  0.06|
|1398457108|344.21| 0.041|
|1398512419|344.21|  0.03|
|1398515509|344.23|  0.03|
|1398515533|344.23|  0.07|
|1398522460|344.21|  0.02|
+----------+------+------+
~~~



